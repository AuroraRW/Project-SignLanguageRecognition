{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "##  sign language recognition demo    ##\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoadModel()\n",
    "model = load_model('model_DL.h5')\n",
    "\n",
    "# make the dictionary to show the alphabet\n",
    "dic={0:'A',1:'B',2:'C',3:'D',4:'E',5:'F',6:'G',7:'H',8:'I',10:'K',\n",
    "     11:'L',12:'M',13:'N',14:'O',15:'P',16:'Q',17:'R',\n",
    "     18:'S',19:'T',20:'U',21:'V',22:'W',23:'X',24:'Y'}\n",
    "\n",
    "# 9 and 25 is the default value\n",
    "dic.setdefault(9)\n",
    "dic.setdefault(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the camera and do the predict for each image\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "w = 360\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "crop_w_start = (width-w)//2\n",
    "crop_h_start = (height-w)//2\n",
    "\n",
    "while True:\n",
    "    # get a frame\n",
    "    ret, frame = cap.read()\n",
    "    # show a frame\n",
    "    frame = frame[crop_h_start:crop_h_start+w, crop_w_start:crop_w_start+w]\n",
    "    img = cv2.flip(frame,1,dst=None)\n",
    "    \n",
    "    #covert to gray\n",
    "    img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #resize\n",
    "    img_test = cv2.resize(img_gray, (28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    #test\n",
    "    img_test = transform.resize(img_test, (28, 28, 1))\n",
    "    img_test = np.expand_dims(img_test, axis=0)\n",
    "\n",
    "    y_pred = model.predict_classes(img_test)\n",
    "    #print(y_pred[0])\n",
    "\n",
    "    text=dic[y_pred[0]]\n",
    "\n",
    "    #output result\n",
    "    cv2.putText(img, text, (25, 25), cv2.FONT_HERSHEY_PLAIN, 2.0, (0, 0, 255), 2) \n",
    "    #show image\n",
    "    cv2.imshow(\"Sign Language V1.0\", img)\n",
    "     \n",
    "    k=cv2.waitKey(1)\n",
    "    if k==ord('q'):\n",
    "        break\n",
    "    elif k==ord(\"s\"):\n",
    "        #save into a file\n",
    "        #cv2.imwrite(\"%s/%d.jpeg\" % (class_name, index),\n",
    "        #            cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA))\n",
    "        cv2.imwrite(\"image000.jpg\" ,cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA))\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
